{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLZcUOG63mWO"
   },
   "source": [
    "# Merge the LoRA adapter into the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.2 in /opt/conda/lib/python3.12/site-packages (4.37.2)\n",
      "Requirement already satisfied: peft==0.7.1 in /opt/conda/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (1.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.37.2\" \"peft==0.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Removing: lora_bias\n",
      "🧹 Removing: layers_to_transform\n",
      "Cleaned config saved to: ../fine_tuned_lora_model/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_adapter_config(config_path):\n",
    "    UNNEEDED_KEYS = [\n",
    "        \"corda_config\",\n",
    "        \"eva_config\",\n",
    "        \"megatron_config\",\n",
    "        \"megatron_core\",\n",
    "        \"loftq_config\",\n",
    "        \"layers_pattern\",\n",
    "        \"layer_replication\",\n",
    "        \"auto_mapping\",\n",
    "        \"revision\",\n",
    "        \"modules_to_save\",\n",
    "        \"trainable_token_indices\",\n",
    "        \"use_dora\",\n",
    "        \"use_rslora\",\n",
    "        \"rank_pattern\",\n",
    "        \"fan_in_fan_out\",\n",
    "        \"init_lora_weights\",\n",
    "        \"exclude_modules\",\n",
    "        \"lora_bias\",\n",
    "        \"layers_to_transform\"\n",
    "    ]\n",
    "\n",
    "    path = Path(config_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    for key in UNNEEDED_KEYS:\n",
    "        if key in config:\n",
    "            print(f\"🧹 Removing: {key}\")\n",
    "            config.pop(key)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    print(f\"Cleaned config saved to: {path}\")\n",
    "\n",
    "# Clean this config before merging LoRA\n",
    "clean_adapter_config(\"../fine_tuned_lora_model/adapter_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e8b282e0ff46e49287c1a7b61c8a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../llama2-legal-merged/tokenizer_config.json',\n",
       " '../llama2-legal-merged/special_tokens_map.json',\n",
       " '../llama2-legal-merged/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "BASE = \"meta-llama/Llama-2-7b-hf\"\n",
    "ADAPTER = \"../fine_tuned_lora_model\"\n",
    "MERGED = pathlib.Path(\"../llama2-legal-merged\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, ADAPTER)\n",
    "\n",
    "# ⚠️ MANUAL LoRA MERGE\n",
    "model.base_model.merge_and_unload()\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE)\n",
    "config.save_pretrained(MERGED)\n",
    "\n",
    "# Save the merged model\n",
    "model.save_pretrained(MERGED, safe_serialization=False)\n",
    "tokenizer.save_pretrained(MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ../llama2-legal-merged/adapter_model.bin ../llama2-legal-merged/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4tu5w3C6chj"
   },
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: optimum[exporters] in /opt/conda/lib/python3.12/site-packages (1.24.0)\n",
      "Requirement already satisfied: transformers>=4.29 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (4.37.2)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (2.5.1+cu124)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (0.31.1)\n",
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.21.1)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.0.15)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.12/site-packages (from onnx) (5.28.3)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.5.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from timm->optimum[exporters]) (0.20.1+cu124)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11->optimum[exporters]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.14)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision->timm->optimum[exporters]) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[exporters] onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EFv44MV748Jb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.02s/it]\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:716: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\t\t-[x] values not close enough, max diff: 0.925936222076416 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 0.925936222076416.\n",
      " The exported model was saved at: ../llama2-legal-onnx\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "  --model ../llama2-legal-merged \\\n",
    "  --task text-generation \\\n",
    "  --dtype fp16 \\\n",
    "  --device cuda \\\n",
    "  --library transformers \\\n",
    "  ../llama2-legal-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G7Rz5tQ6fCr"
   },
   "source": [
    "# Graph-optimise & kernel-fuse (Didnot Run for just checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXcrWtCj5ADx"
   },
   "outputs": [],
   "source": [
    "python -m onnxruntime_tools.optimizer_cli \\\n",
    "       --input LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder.onnx \\\n",
    "       --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
    "       --float16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjkeXn7x6lXd"
   },
   "source": [
    "# (Optional) INT-4 / INT-8 quantisation (Didnot Run for just checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0pqVRsd6Sx-"
   },
   "outputs": [],
   "source": [
    "pip install neural-compressor\n",
    "\n",
    "inc_quantizer \\\n",
    "  --model LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
    "  --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnxmodel_decoder_int4.onnx \\\n",
    "  --approach static  --performance-only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWvrsduA6tg7"
   },
   "source": [
    "# Quick test in ONNX Runtime (Dont Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-ypBLHI6UMJ"
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort, numpy as np, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/home/cc/models/llama2-legal-merged\")\n",
    "sess = ort.InferenceSession(\n",
    "          \"/home/cc/models/llama2-legal-onnx/model_decoder_int4.onnx\",\n",
    "          providers=[\"TensorrtExecutionProvider\",\"CUDAExecutionProvider\"])\n",
    "\n",
    "prompt = tok(\"One‑sentence summary of clause 7.2:\", return_tensors=\"np\")\n",
    "outputs = sess.run(None, {\"input_ids\":prompt[\"input_ids\"],\n",
    "                          \"attention_mask\":prompt[\"attention_mask\"]})\n",
    "print(outputs[0].shape)     # sanity: (1, seq_len, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (24.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime -y\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: "
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y nvidia-cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: CPUExecutionProvider\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     29\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (1, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     next_token_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:270\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama2-legal-merged\")\n",
    "\n",
    "# Set up ONNX Runtime session with fallback\n",
    "providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "available_providers = ort.get_available_providers()\n",
    "sess = ort.InferenceSession(\n",
    "    \"../llama2-legal-onnx/model.onnx\",\n",
    "    providers=[p for p in providers if p in available_providers]\n",
    ")\n",
    "\n",
    "print(\"Using provider:\", sess.get_providers()[0])\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"One-sentence summary of clause 7.2:\"\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Start generation loop\n",
    "max_new_tokens = 50\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    position_ids = np.arange(input_ids.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    outputs = sess.run(None, {\n",
    "        \"input_ids\": input_ids.astype(np.int64),\n",
    "        \"attention_mask\": attention_mask.astype(np.int64),\n",
    "        \"position_ids\": position_ids\n",
    "    })\n",
    "\n",
    "    logits = outputs[0]  # (1, seq_len, vocab_size)\n",
    "    next_token_id = np.argmax(logits[:, -1, :], axis=-1)\n",
    "\n",
    "    # Append next token\n",
    "    input_ids = np.concatenate([input_ids, next_token_id[:, None]], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token_id)[:, None]], axis=1)\n",
    "\n",
    "    if next_token_id[0] == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-M1R7Su6wJo"
   },
   "source": [
    "#Wrap with FastAPI or Triton(Dont Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTEsN_6Z6WQ1"
   },
   "outputs": [],
   "source": [
    "docker run -d --gpus all -p 8000:8000 \\\n",
    "  -v /home/cc/triton_repo:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:24.05-py3 \\\n",
    "  tritonserver --model-repository=/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx5IqTOGThcA"
   },
   "source": [
    "#Build a FastAPI ONNX micro-service (pattern from the hand-out)\n",
    "````\n",
    "docker compose -f docker-compose-fastapi.yaml up -d --build\n",
    "````\n",
    "\n",
    "````\n",
    "curl -X POST http://<IP>:8000/generate \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"prompt\":\"Summarise clause 7.2 in two lines\"}'\n",
    "````\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
