{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Merge the LoRA adapter into the base model"
      ],
      "metadata": {
        "id": "YLZcUOG63mWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d4upfPJvnGf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, merge_and_unload\n",
        "import pathlib, torch\n",
        "\n",
        "BASE    = \"meta-llama/Llama-2-7b-hf\"\n",
        "ADAPTER = \"LLM_LegalDocSummarization/fine_tuned_lora_model/\"\n",
        "MERGED  = pathlib.Path(\"LLM_LegalDocSummarization/llama2-legal-merged\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, ADAPTER)\n",
        "model = merge_and_unload(model)          # <-- fuse LoRA matrices\n",
        "model.save_pretrained(MERGED)\n",
        "AutoTokenizer.from_pretrained(BASE).save_pretrained(MERGED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Export to ONNX"
      ],
      "metadata": {
        "id": "T4tu5w3C6chj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optimum[exporters] onnx onnxruntime-gpu\n",
        "\n",
        "optimum-cli export onnx \\\n",
        "    --model LLM_LegalDocSummarization/llama2-legal-merged \\\n",
        "    --task text-generation \\\n",
        "    --fp16 \\\n",
        "    LLM_LegalDocSummarization/llama2-legal-onnx"
      ],
      "metadata": {
        "id": "EFv44MV748Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph-optimise & kernel-fuse"
      ],
      "metadata": {
        "id": "3G7Rz5tQ6fCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python -m onnxruntime_tools.optimizer_cli \\\n",
        "       --input LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder.onnx \\\n",
        "       --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
        "       --float16\n"
      ],
      "metadata": {
        "id": "hXcrWtCj5ADx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(Optional) INT-4 / INT-8 quantisation (testing)"
      ],
      "metadata": {
        "id": "JjkeXn7x6lXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neural-compressor\n",
        "\n",
        "inc_quantizer \\\n",
        "  --model LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
        "  --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnxmodel_decoder_int4.onnx \\\n",
        "  --approach static  --performance-only\n"
      ],
      "metadata": {
        "id": "r0pqVRsd6Sx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick test in ONNX Runtime (Dont Run)"
      ],
      "metadata": {
        "id": "DWvrsduA6tg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort, numpy as np, torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"/home/cc/models/llama2-legal-merged\")\n",
        "sess = ort.InferenceSession(\n",
        "          \"/home/cc/models/llama2-legal-onnx/model_decoder_int4.onnx\",\n",
        "          providers=[\"TensorrtExecutionProvider\",\"CUDAExecutionProvider\"])\n",
        "\n",
        "prompt = tok(\"One‑sentence summary of clause 7.2:\", return_tensors=\"np\")\n",
        "outputs = sess.run(None, {\"input_ids\":prompt[\"input_ids\"],\n",
        "                          \"attention_mask\":prompt[\"attention_mask\"]})\n",
        "print(outputs[0].shape)     # sanity: (1, seq_len, vocab)"
      ],
      "metadata": {
        "id": "T-ypBLHI6UMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wrap with FastAPI or Triton(Dont Run)"
      ],
      "metadata": {
        "id": "P-M1R7Su6wJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker run -d --gpus all -p 8000:8000 \\\n",
        "  -v /home/cc/triton_repo:/models \\\n",
        "  nvcr.io/nvidia/tritonserver:24.05-py3 \\\n",
        "  tritonserver --model-repository=/models\n"
      ],
      "metadata": {
        "id": "HTEsN_6Z6WQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build a FastAPI ONNX micro-service (pattern from the hand-out)\n",
        "````\n",
        "docker compose -f docker-compose-fastapi.yaml up -d --build\n",
        "````\n",
        "\n",
        "````\n",
        "curl -X POST http://<IP>:8000/generate \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -d '{\"prompt\":\"Summarise clause 7.2 in two lines\"}'\n",
        "````\n"
      ],
      "metadata": {
        "id": "Dx5IqTOGThcA"
      }
    }
  ]
}