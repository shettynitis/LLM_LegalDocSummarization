{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLZcUOG63mWO"
   },
   "source": [
    "# Merge the LoRA adapter into the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.2 in /opt/conda/lib/python3.12/site-packages (4.37.2)\n",
      "Requirement already satisfied: peft==0.7.1 in /opt/conda/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (1.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.37.2\" \"peft==0.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned config saved to: ../fine_tuned_lora_model/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_adapter_config(config_path):\n",
    "    UNNEEDED_KEYS = [\n",
    "        \"corda_config\",\n",
    "        \"eva_config\",\n",
    "        \"megatron_config\",\n",
    "        \"megatron_core\",\n",
    "        \"loftq_config\",\n",
    "        \"layers_pattern\",\n",
    "        \"layer_replication\",\n",
    "        \"auto_mapping\",\n",
    "        \"revision\",\n",
    "        \"modules_to_save\",\n",
    "        \"trainable_token_indices\",\n",
    "        \"use_dora\",\n",
    "        \"use_rslora\",\n",
    "        \"rank_pattern\",\n",
    "        \"fan_in_fan_out\",\n",
    "        \"init_lora_weights\",\n",
    "        \"exclude_modules\",\n",
    "        \"lora_bias\",\n",
    "        \"layers_to_transform\"\n",
    "    ]\n",
    "\n",
    "    path = Path(config_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    for key in UNNEEDED_KEYS:\n",
    "        if key in config:\n",
    "            print(f\"🧹 Removing: {key}\")\n",
    "            config.pop(key)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    print(f\"Cleaned config saved to: {path}\")\n",
    "\n",
    "# Clean this config before merging LoRA\n",
    "clean_adapter_config(\"../fine_tuned_lora_model/adapter_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7ce15642b647bab75c904358126ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca2e673ea54be8aee0c62c1f399cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a540470d03cf41fba2ad8b13128264c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8285b7443574a5f8327ec70a3c1f994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c780ed7bf1f348baad0197c962379a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48491fb78c274c4ba89852cc153cca31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763a641b9caa413e856f06fa38bfc375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5e9abb960146748d1301865633db94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a117ca918f714da89e43bd5b449019ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff9cbf9e39f4392bc1129bc4a997d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec8bbfaa42743908ad5a74635552952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../llama2-legal-merged/tokenizer_config.json',\n",
       " '../llama2-legal-merged/special_tokens_map.json',\n",
       " '../llama2-legal-merged/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "BASE = \"meta-llama/Llama-2-7b-hf\"\n",
    "ADAPTER = \"../fine_tuned_lora_model\"\n",
    "MERGED = pathlib.Path(\"../llama2-legal-merged\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, ADAPTER)\n",
    "\n",
    "# ⚠️ MANUAL LoRA MERGE\n",
    "model.base_model.merge_and_unload()\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE)\n",
    "config.save_pretrained(MERGED)\n",
    "\n",
    "# Save the merged model\n",
    "model.save_pretrained(MERGED, safe_serialization=False)\n",
    "tokenizer.save_pretrained(MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mv ../llama2-legal-merged/adapter_model.bin ../llama2-legal-merged/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4tu5w3C6chj"
   },
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Requirement already satisfied: optimum[exporters] in /opt/conda/lib/python3.12/site-packages (1.24.0)\n",
      "Requirement already satisfied: transformers>=4.29 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (4.37.2)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (2.5.1+cu124)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (0.31.1)\n",
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.22.0)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.0.15)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.12/site-packages (from onnx) (5.28.3)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.5.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from timm->optimum[exporters]) (0.20.1+cu124)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11->optimum[exporters]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.14)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision->timm->optimum[exporters]) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[exporters] onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EFv44MV748Jb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:716: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\t\t-[x] values not close enough, max diff: 0.8278255462646484 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 0.8278255462646484.\n",
      " The exported model was saved at: ../llama2-legal-onnx\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "  --model ../llama2-legal-merged \\\n",
    "  --task text-generation \\\n",
    "  --dtype fp16 \\\n",
    "  --device cuda \\\n",
    "  --library transformers \\\n",
    "  ../llama2-legal-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWvrsduA6tg7"
   },
   "source": [
    "# Quick test in ONNX Runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for text-generation with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: CPUExecutionProvider\n",
      "\n",
      "Generated Text:\n",
      "One-sentence summary of clause 7.2:\n",
      "The contractor shall not be liable for any loss or damage whatsoever caused by any\n",
      "\n",
      "--- Inference Benchmark (Total Tokens: 20) ---\n",
      "Total time: 24.37s\n",
      "Avg per token: 1218.70 ms\n",
      "Median: 1167.67 ms | 95th: 1564.73 ms | 99th: 1585.26 ms\n",
      "Throughput: 0.82 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama2-legal-merged\")\n",
    "\n",
    "# Load ONNX model using available GPU execution provider\n",
    "available_providers = ort.get_available_providers()\n",
    "ort_session = ort.InferenceSession(\n",
    "    \"../llama2-legal-onnx/model.onnx\",  # Use optimized .onnx\n",
    "     providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "print(\"Using provider:\", ort_session.get_providers()[0])\n",
    "\n",
    "# Prompt setup\n",
    "prompt_text = \"One-sentence summary of clause 7.2:\"\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Generate tokens\n",
    "max_new_tokens = 20\n",
    "times = []\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    position_ids = np.arange(input_ids.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = ort_session.run(None, {\n",
    "        \"input_ids\": input_ids.astype(np.int64),\n",
    "        \"attention_mask\": attention_mask.astype(np.int64),\n",
    "        \"position_ids\": position_ids\n",
    "    })\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    next_token = np.argmax(logits[:, -1, :], axis=-1)\n",
    "    input_ids = np.concatenate([input_ids, next_token[:, None]], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token)[:, None]], axis=1)\n",
    "\n",
    "    if next_token[0] == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Benchmarking\n",
    "print(f\"\\n--- Inference Benchmark (Total Tokens: {len(times)}) ---\")\n",
    "print(f\"Total time: {sum(times):.2f}s\")\n",
    "print(f\"Avg per token: {np.mean(times)*1000:.2f} ms\")\n",
    "print(f\"Median: {np.percentile(times, 50)*1000:.2f} ms | 95th: {np.percentile(times, 95)*1000:.2f} ms | 99th: {np.percentile(times, 99)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {len(times)/sum(times):.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Inference for text-generation with past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastAPI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mort\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import onnxruntime as ort, numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1) set the TensorRT flags *before* session creation\n",
    "import os\n",
    "os.environ[\"ORT_TENSORRT_FP16_ENABLE\"] = \"1\"\n",
    "os.environ[\"ORT_TENSORRT_ENGINE_CACHE_ENABLE\"] = \"1\"\n",
    "os.environ[\"ORT_TENSORRT_ENGINE_CACHE_PATH\"] = \"/cache\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama2-legal-merged\")\n",
    "sess = ort.InferenceSession(\n",
    "        \"./llama-model-onnx/model.onnx\",\n",
    "        providers=[\"TensorrtExecutionProvider\",\"CUDAExecutionProvider\"])\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Req(BaseModel):\n",
    "    prompt: str\n",
    "    max_new_tokens: int = 100\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(req: Req):\n",
    "    ids = tokenizer(req.prompt, return_tensors=\"np\")\n",
    "    input_ids, attn = ids[\"input_ids\"], ids[\"attention_mask\"]\n",
    "\n",
    "    for _ in range(req.max_new_tokens):\n",
    "        pos = np.arange(input_ids.shape[1], dtype=np.int64)[None, :]\n",
    "        out  = sess.run(None, {\n",
    "                \"input_ids\": input_ids.astype(np.int64),\n",
    "                \"attention_mask\": attn.astype(np.int64),\n",
    "                \"position_ids\": pos})\n",
    "        next_id = np.argmax(out[0][:, -1, :], axis=-1)\n",
    "        input_ids = np.concatenate([input_ids, next_id[:, None]], axis=1)\n",
    "        attn      = np.concatenate([attn, np.ones_like(next_id)[:, None]], axis=1)\n",
    "        if next_id[0] == tokenizer.eos_token_id: break\n",
    "\n",
    "    return {\"text\": tokenizer.decode(input_ids[0], skip_special_tokens=True)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-M1R7Su6wJo"
   },
   "source": [
    "#Wrap with FastAPI or Triton(Dont Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTEsN_6Z6WQ1"
   },
   "outputs": [],
   "source": [
    "docker run -d --gpus all -p 8000:8000 \\\n",
    "  -v /home/cc/triton_repo:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:24.05-py3 \\\n",
    "  tritonserver --model-repository=/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 03:06:40.724437322 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 32 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:06:40.736667639 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:06:40.736674633 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: CUDAExecutionProvider\n",
      "\n",
      "Generated Text:\n",
      "Give summary of clause 7.2:\n",
      "The clause 7.2 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.3:\n",
      "The clause 7.3 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.4: The clause 7.4 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.5: The clause 7.5 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.6: The clause 7.6 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.7: The clause 7.7 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.8: The clause 7.8 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.9: The clause 7.9 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.10: The clause 7.10 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice.\n",
      "Give summary of clause 7.11: The clause 7.11 of the contract is related to the payment of the contractor. The contractor is required to pay the subcontractor within 14 days of the receipt of the invoice. The contractor is also required to pay the subcontractor within 14 days of the receipt\n",
      "\n",
      "--- Inference Benchmark (Total Tokens: 1000) ---\n",
      "Total time: 67.30s\n",
      "Avg per token: 67.30 ms\n",
      "Median: 63.87 ms | 95th: 122.28 ms | 99th: 127.15 ms\n",
      "Throughput: 14.86 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama2-legal-merged\")\n",
    "\n",
    "# Load ONNX model using available GPU execution provider\n",
    "available_providers = ort.get_available_providers()\n",
    "ort_session = ort.InferenceSession(\n",
    "    \"../llama2-legal-onnx/model.onnx\",  # Use optimized .onnx\n",
    "     providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "print(\"Using provider:\", ort_session.get_providers()[0])\n",
    "\n",
    "# Prompt setup\n",
    "prompt_text = \"Give summary of clause 7.2:\"\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Generate tokens\n",
    "max_new_tokens = 1000\n",
    "times = []\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    position_ids = np.arange(input_ids.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = ort_session.run(None, {\n",
    "        \"input_ids\": input_ids.astype(np.int64),\n",
    "        \"attention_mask\": attention_mask.astype(np.int64),\n",
    "        \"position_ids\": position_ids\n",
    "    })\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    next_token = np.argmax(logits[:, -1, :], axis=-1)\n",
    "    input_ids = np.concatenate([input_ids, next_token[:, None]], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token)[:, None]], axis=1)\n",
    "\n",
    "    if next_token[0] == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Benchmarking\n",
    "print(f\"\\n--- Inference Benchmark (Total Tokens: {len(times)}) ---\")\n",
    "print(f\"Total time: {sum(times):.2f}s\")\n",
    "print(f\"Avg per token: {np.mean(times)*1000:.2f} ms\")\n",
    "print(f\"Median: {np.percentile(times, 50)*1000:.2f} ms | 95th: {np.percentile(times, 95)*1000:.2f} ms | 99th: {np.percentile(times, 99)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {len(times)/sum(times):.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 03:07:51.316876084 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 32 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:07:51.329300097 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:07:51.329307461 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX provider → CUDAExecutionProvider\n",
      "\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Give a concise summary of clause 7.2:\n",
      "The contractor can request from the client that they be released from their obligations under the contract, provided certain criteria are fulfilled. The parties must also agree to the new terms and conditions when this happens.\n",
      "What is ‘the subject matter’ of the contract? (Clause 1)\n",
      "This clause sets out what the subject matter of the contract is in relation to the works – that is, for example, an extension or refurbishment project. In the case of services, it will be whatever work the service provider does on behalf of the other party. This could include accountancy, marketing, IT support etc.\n",
      "Does the contract require specific performance by either party?\n",
      "No, although some clauses do refer to ‘specific performance not being an appropriate remedy.’ For instance, if one of the parties to a construction contract was unable to complete their own building site, then there would probably only be an argument about whether compensation should be paid instead of requiring them to start again from scratch. Some employment related legal documents may have provisions whereby they require people who breach their contractual duties to perform them anyway without any penalty. For example, if someone fails to turn up at work following illness but continues claiming sick\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Benchmark (256 new tokens) ---\n",
      "total 7.27s | mean 28.4 ms | 95th 40.2 ms | throughput 35.2 tok/s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fast-ish ONNX‑runtime decoding with basic top‑k sampling\n",
    "and a repetition penalty to avoid infinite loops.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ─────────── Config ────────────\n",
    "MODEL_DIR      = \"../llama2-legal-onnx/model.onnx\"\n",
    "TOKENIZER_DIR  = \"../llama2-legal-merged\"\n",
    "PROMPT_TEXT    = \"Give a concise summary of clause 7.2:\"\n",
    "MAX_NEW_TOKENS = 256                # hard cap\n",
    "TEMPERATURE    = 0.8\n",
    "TOP_K          = 40\n",
    "REPETITION_PEN = 1.15               # >1.0 penalises already‑seen tokens\n",
    "END_TOKENS     = {0, 2, 50256}      # eos, or add your own\n",
    "# ───────────────────────────────\n",
    "\n",
    "def sample_top_k(logits, top_k, temperature=1.0):\n",
    "    \"\"\"Return one sampled token id (numpy int64) from top‑k\"\"\"\n",
    "    logits = logits.astype(np.float32) / temperature\n",
    "    # keep top‑k\n",
    "    if top_k and top_k < logits.size:\n",
    "        top_ids = logits.argsort()[-top_k:]\n",
    "        mask = np.ones_like(logits, dtype=bool)\n",
    "        mask[top_ids] = False\n",
    "        logits[mask] = -np.inf\n",
    "    probs = np.exp(logits - np.max(logits))\n",
    "    probs /= probs.sum()\n",
    "    return np.random.choice(len(logits), p=probs)\n",
    "\n",
    "# ─────────── Load model ─────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "sess = ort.InferenceSession(\n",
    "    MODEL_DIR,\n",
    "    providers=[\"CUDAExecutionProvider\"],  # assumes GPU\n",
    ")\n",
    "print(\"ONNX provider →\", sess.get_providers()[0])\n",
    "\n",
    "# ─────────── Prepare prompt ─────\n",
    "inputs          = tokenizer(PROMPT_TEXT, return_tensors=\"np\")\n",
    "input_ids       = inputs[\"input_ids\"]\n",
    "attention_mask  = inputs[\"attention_mask\"]\n",
    "\n",
    "generated = input_ids.copy()\n",
    "times = []\n",
    "\n",
    "# ─────────── Decode loop ────────\n",
    "for _ in range(MAX_NEW_TOKENS):\n",
    "    position_ids = np.arange(generated.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    start = time.time()\n",
    "    logits = sess.run(\n",
    "        None,\n",
    "        {\n",
    "            \"input_ids\": generated.astype(np.int64),\n",
    "            \"attention_mask\": attention_mask.astype(np.int64),\n",
    "            \"position_ids\": position_ids,\n",
    "        },\n",
    "    )[0]\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "    # repetition penalty\n",
    "    logits[0, -1, np.unique(generated)] /= REPETITION_PEN\n",
    "\n",
    "    next_id = sample_top_k(logits[0, -1], top_k=TOP_K, temperature=TEMPERATURE)\n",
    "    if next_id in END_TOKENS:\n",
    "        break\n",
    "\n",
    "    next_token = np.array([[next_id]], dtype=np.int64)\n",
    "    generated  = np.concatenate([generated, next_token], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token)], axis=1)\n",
    "\n",
    "# ─────────── Output + stats ─────\n",
    "text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated text:\\n\" + \"-\"*60 + f\"\\n{text}\\n\" + \"-\"*60)\n",
    "\n",
    "n = len(times)\n",
    "print(f\"\\n--- Benchmark ({n} new tokens) ---\")\n",
    "print(f\"total {sum(times):.2f}s | mean {np.mean(times)*1000:.1f} ms \"\n",
    "      f\"| 95th {np.percentile(times,95)*1000:.1f} ms \"\n",
    "      f\"| throughput {n/sum(times):.1f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx5IqTOGThcA"
   },
   "source": [
    "#Build a FastAPI ONNX micro-service (pattern from the hand-out)\n",
    "````\n",
    "docker compose -f docker-compose-fastapi.yaml up -d --build\n",
    "````\n",
    "\n",
    "````\n",
    "curl -X POST http://<IP>:8000/generate \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"prompt\":\"Summarise clause 7.2 in two lines\"}'\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 03:09:22.978709284 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 32 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:09:22.991289654 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 03:09:22.991297429 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX provider → CUDAExecutionProvider\n",
      "\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Summarise clause 7.2 in two lines\n",
      "A summary is a paraphrase of the text, that is, an outline of the main themes with no specific wording from the original passage; its purpose is to allow you to see the important points very quickly and concisely. In an essay examination you will not be allowed more than 150 words for your summaries (although there are some exceptions). Here we set out an example of how such a short piece can look when written as if it was part of an essay.\n",
      "Idea: Describe clause 7.2 on the effectiveness of advertising by showing what this means in practice and explaining why advertisements have their intended effects. You should conclude that advertisers use images which appeal directly to our emotions rather than appeal to reason or logic.\n",
      "We know immediately from reading 'effective' what clause 7.2 is about. The second sentence tells us what sort of advertising is effective (i.e. has direct impacts): we see the target audience here as children who don’t understand complex arguments based on logic but respond purely through emotional reaction. The third sentence reveals 'successful' as another term associated with advertising and this gives us clues\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Benchmark (256 new tokens) ---\n",
      "total 7.19s | mean 28.1 ms | 95th 39.4 ms | throughput 35.6 tok/s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fast-ish ONNX‑runtime decoding with basic top‑k sampling\n",
    "and a repetition penalty to avoid infinite loops.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ─────────── Config ────────────\n",
    "MODEL_DIR      = \"../llama2-legal-onnx/model.onnx\"\n",
    "TOKENIZER_DIR  = \"../llama2-legal-merged\"\n",
    "PROMPT_TEXT    = \"Summarise clause 7.2 in two lines\"#\"Give a concise summary of clause 7.2:\"\n",
    "MAX_NEW_TOKENS = 256                # hard cap\n",
    "TEMPERATURE    = 0.8\n",
    "TOP_K          = 40\n",
    "REPETITION_PEN = 1.15               # >1.0 penalises already‑seen tokens\n",
    "END_TOKENS     = {0, 2, 50256}      # eos, or add your own\n",
    "# ───────────────────────────────\n",
    "\n",
    "def sample_top_k(logits, top_k, temperature=1.0):\n",
    "    \"\"\"Return one sampled token id (numpy int64) from top‑k\"\"\"\n",
    "    logits = logits.astype(np.float32) / temperature\n",
    "    # keep top‑k\n",
    "    if top_k and top_k < logits.size:\n",
    "        top_ids = logits.argsort()[-top_k:]\n",
    "        mask = np.ones_like(logits, dtype=bool)\n",
    "        mask[top_ids] = False\n",
    "        logits[mask] = -np.inf\n",
    "    probs = np.exp(logits - np.max(logits))\n",
    "    probs /= probs.sum()\n",
    "    return np.random.choice(len(logits), p=probs)\n",
    "\n",
    "# ─────────── Load model ─────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "sess = ort.InferenceSession(\n",
    "    MODEL_DIR,\n",
    "    providers=[\"CUDAExecutionProvider\"],  # assumes GPU\n",
    ")\n",
    "print(\"ONNX provider →\", sess.get_providers()[0])\n",
    "\n",
    "# ─────────── Prepare prompt ─────\n",
    "inputs          = tokenizer(PROMPT_TEXT, return_tensors=\"np\")\n",
    "input_ids       = inputs[\"input_ids\"]\n",
    "attention_mask  = inputs[\"attention_mask\"]\n",
    "\n",
    "generated = input_ids.copy()\n",
    "times = []\n",
    "\n",
    "# ─────────── Decode loop ────────\n",
    "for _ in range(MAX_NEW_TOKENS):\n",
    "    position_ids = np.arange(generated.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    start = time.time()\n",
    "    logits = sess.run(\n",
    "        None,\n",
    "        {\n",
    "            \"input_ids\": generated.astype(np.int64),\n",
    "            \"attention_mask\": attention_mask.astype(np.int64),\n",
    "            \"position_ids\": position_ids,\n",
    "        },\n",
    "    )[0]\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "    # repetition penalty\n",
    "    logits[0, -1, np.unique(generated)] /= REPETITION_PEN\n",
    "\n",
    "    next_id = sample_top_k(logits[0, -1], top_k=TOP_K, temperature=TEMPERATURE)\n",
    "    if next_id in END_TOKENS:\n",
    "        break\n",
    "\n",
    "    next_token = np.array([[next_id]], dtype=np.int64)\n",
    "    generated  = np.concatenate([generated, next_token], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token)], axis=1)\n",
    "\n",
    "# ─────────── Output + stats ─────\n",
    "text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated text:\\n\" + \"-\"*60 + f\"\\n{text}\\n\" + \"-\"*60)\n",
    "\n",
    "n = len(times)\n",
    "print(f\"\\n--- Benchmark ({n} new tokens) ---\")\n",
    "print(f\"total {sum(times):.2f}s | mean {np.mean(times)*1000:.1f} ms \"\n",
    "      f\"| 95th {np.percentile(times,95)*1000:.1f} ms \"\n",
    "      f\"| throughput {n/sum(times):.1f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-skinny==2.22.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.12/site-packages (from mlflow) (3.1.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.12/site-packages (from mlflow) (1.14.1)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.12/site-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /opt/conda/lib/python3.12/site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.12/site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from mlflow) (19.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.12/site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.12/site-packages (from mlflow) (1.15.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from mlflow) (2.0.37)\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading databricks_sdk-0.52.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (8.6.1)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (5.28.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (2.10.6)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (4.12.2)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Collecting Werkzeug>=3.1 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.2 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/conda/lib/python3.12/site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3->mlflow) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.2.18)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (2024.12.14)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlflow) (0.14.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (5.0.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (4.8.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (1.3.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading databricks_sdk-0.52.0-py3-none-any.whl (700 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.33.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl (194 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: Werkzeug, uvicorn, sqlparse, pyasn1, markdown, itsdangerous, gunicorn, graphql-core, cachetools, starlette, rsa, pyasn1-modules, opentelemetry-api, graphql-relay, Flask, docker, opentelemetry-semantic-conventions, graphene, google-auth, fastapi, opentelemetry-sdk, databricks-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-3.1.0 Werkzeug-3.1.3 cachetools-5.5.2 databricks-sdk-0.52.0 docker-7.1.0 fastapi-0.115.12 google-auth-2.40.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 itsdangerous-2.2.0 markdown-3.8 mlflow-2.22.0 mlflow-skinny-2.22.0 opentelemetry-api-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 sqlparse-0.5.3 starlette-0.46.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://129.114.25.240:8000\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 03:18:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import mlflow\n",
    "import mlflow.onnx\n",
    "\n",
    "run_name         = \"legalsummarizer\"\n",
    "artifact_path    = \"onnx_model\"\n",
    "local_onnx_path  = \"../llama2-legal-onnx/model.onnx\"   # your file\n",
    "\n",
    "# Load ONNX into memory (so we can log without copying the file twice)\n",
    "model_proto = onnx.load(local_onnx_path)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # (optional) meta‑data\n",
    "    mlflow.log_params({\n",
    "        \"base_model\": \"Llama‑2‑7B\",\n",
    "        \"task\": \"legal_clause_summarization\",\n",
    "        \"quantized\": \"fp16\"\n",
    "    })\n",
    "\n",
    "    # Log the ONNX model — this creates the MLflow *model* directory + MLmodel file\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model=model_proto,\n",
    "        artifact_path=artifact_path,\n",
    "        signature=None,           # add an example in/out signature if you like\n",
    "        input_example=None\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(\"Logged to run:\", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import register_model\n",
    "\n",
    "model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "registered_model = register_model(model_uri, \"LegalClauseSummarizer\")\n",
    "\n",
    "print(\"Registered name:\", registered_model.name)\n",
    "print(\"Version:\", registered_model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
