version: "3.9"

services:
  llm-api:                              # FASTAPI + ONNX runtime
    build:
      context: .                        # project root is build context
      dockerfile: Dockerfile.fastapi           # exact file name/path
    container_name: llm-api
    environment:
      # Where your model is stored in MLflow (edit to your run or alias)
      MODEL_URI: "s3://mlflow-artifacts"
      # Where the tracking / registry server lives
      MLFLOW_TRACKING_URI: "http://129.114.25.240:8000"
      # Optional: S3/MinIO creds if artifacts are remote
      AWS_ACCESS_KEY_ID:   "your-access-key"
      AWS_SECRET_ACCESS_KEY: "your-secret-key"
      MLFLOW_S3_ENDPOINT_URL: "http://129.114.25.240:9000"
    ports:
      - "7000:7000"                     # host → container
    restart: unless-stopped

  
  notebook:                            # Jupyter + CUDA/TensorRT
    build:
      context: .
      dockerfile: Dockerfile.jupyter-onnx-gpu
    container_name: notebook
    environment:
      MLFLOW_TRACKING_URI: "https://mlflow.my‑org.com"
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work   # keep your work persistent
    restart: unless-stopped
    depends_on:
      - llm-api                         # optional, remove if not needed
