{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLZcUOG63mWO"
   },
   "source": [
    "# Merge the LoRA adapter into the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.2\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting peft==0.7.1\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.37.2)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.37.2)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.37.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (2.5.1+cu124)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.7.1)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed accelerate-1.6.0 hf-xet-1.1.0 huggingface-hub-0.31.1 peft-0.7.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.15.2 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.37.2\" \"peft==0.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned config saved to: ../fine_tuned_lora_model/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_adapter_config(config_path):\n",
    "    UNNEEDED_KEYS = [\n",
    "        \"corda_config\",\n",
    "        \"eva_config\",\n",
    "        \"megatron_config\",\n",
    "        \"megatron_core\",\n",
    "        \"loftq_config\",\n",
    "        \"layers_pattern\",\n",
    "        \"layer_replication\",\n",
    "        \"auto_mapping\",\n",
    "        \"revision\",\n",
    "        \"modules_to_save\",\n",
    "        \"trainable_token_indices\",\n",
    "        \"use_dora\",\n",
    "        \"use_rslora\",\n",
    "        \"rank_pattern\",\n",
    "        \"fan_in_fan_out\",\n",
    "        \"init_lora_weights\",\n",
    "        \"exclude_modules\",\n",
    "        \"lora_bias\",\n",
    "        \"layers_to_transform\"\n",
    "    ]\n",
    "\n",
    "    path = Path(config_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    for key in UNNEEDED_KEYS:\n",
    "        if key in config:\n",
    "            print(f\"üßπ Removing: {key}\")\n",
    "            config.pop(key)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    print(f\"Cleaned config saved to: {path}\")\n",
    "\n",
    "# Clean this config before merging LoRA\n",
    "clean_adapter_config(\"../fine_tuned_lora_model/adapter_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76b9797b2dd42cca0793833d594e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b799c8a4247742988b193be475f06367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da207e67ff4eeea1fe9c93c3353a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26d8f9607b24a61ba14cf301e85a827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3382506bf1b4dadb84c2ebc30fc1955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc74c0760d3d4be0bc6ca0f035a1f966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44e6f55f73f419a80e1386301631dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6921ec355d41a79e06261c9d6bef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8921b7ec1e44b7b7b036c13d60aaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a7aaf66bdc42299a5d5abbbfabf034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aabae79e47d470dba8225e7420c5b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../llama2-legal-merged/tokenizer_config.json',\n",
       " '../llama2-legal-merged/special_tokens_map.json',\n",
       " '../llama2-legal-merged/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "BASE = \"meta-llama/Llama-2-7b-hf\"\n",
    "ADAPTER = \"../fine_tuned_lora_model\"\n",
    "MERGED = pathlib.Path(\"../llama2-legal-merged\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, ADAPTER)\n",
    "\n",
    "# ‚ö†Ô∏è MANUAL LoRA MERGE\n",
    "model.base_model.merge_and_unload()\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE)\n",
    "config.save_pretrained(MERGED)\n",
    "\n",
    "# Save the merged model\n",
    "model.save_pretrained(MERGED, safe_serialization=False)\n",
    "tokenizer.save_pretrained(MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ../llama2-legal-merged/adapter_model.bin ../llama2-legal-merged/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4tu5w3C6chj"
   },
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Collecting optimum[exporters]\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: transformers>=4.29 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (4.37.2)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (2.5.1+cu124)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (0.31.1)\n",
      "Collecting onnxruntime (from optimum[exporters])\n",
      "  Downloading onnxruntime-1.22.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting timm (from optimum[exporters])\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.12/site-packages (from onnx) (5.28.3)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.5.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from timm->optimum[exporters]) (0.20.1+cu124)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11->optimum[exporters]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.14)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision->timm->optimum[exporters]) (11.1.0)\n",
      "Downloading onnxruntime-1.22.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: onnxruntime, timm, optimum\n",
      "Successfully installed onnxruntime-1.22.0 optimum-1.24.0 timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[exporters] onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting tensorrt-llm==0.19.0\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.19.0-cp312-cp312-linux_x86_64.whl (2049.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: accelerate>=0.25.0 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (1.6.0)\n",
      "Collecting build (from tensorrt-llm==0.19.0)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting colored (from tensorrt-llm==0.19.0)\n",
      "  Downloading colored-2.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting cuda-python (from tensorrt-llm==0.19.0)\n",
      "  Downloading cuda_python-12.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting diffusers>=0.27.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting lark (from tensorrt-llm==0.19.0)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting mpi4py (from tensorrt-llm==0.19.0)\n",
      "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
      "  Installing build dependencies ... \u001bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (1.17.0)\n",
      "Collecting onnx_graphsurgeon>=0.5.2 (from tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
      "Collecting openai (from tensorrt-llm==0.19.0)\n",
      "  Downloading openai-1.78.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting polygraphy (from tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/polygraphy/polygraphy-0.49.20-py2.py3-none-any.whl (354 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (6.1.1)\n",
      "Collecting nvidia-ml-py>=12 (from tensorrt-llm==0.19.0)\n",
      "  Downloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pynvml>=12.0.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pulp (from tensorrt-llm==0.19.0)\n",
      "  Downloading pulp-3.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (2.2.3)\n",
      "Requirement already satisfied: h5py==3.12.1 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (3.12.1)\n",
      "Collecting StrEnum (from tensorrt-llm==0.19.0)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from tensorrt-llm==0.19.0)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tensorrt~=10.9.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-10.9.0.34.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<=2.7.0a0,>=2.6.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (0.20.1+cu124)\n",
      "Collecting nvidia-modelopt~=0.27.0 (from nvidia-modelopt[torch]~=0.27.0->tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.27.1-py3-none-manylinux_2_28_x86_64.whl (658 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m658.2/658.2 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (12.4.127)\n",
      "Collecting transformers==4.51.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: pydantic>=2.9.1 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (2.10.6)\n",
      "Collecting pillow==10.3.0 (from tensorrt-llm==0.19.0)\n",
      "  Downloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: wheel<=0.45.1 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (0.45.1)\n",
      "Requirement already satisfied: optimum in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (1.24.0)\n",
      "Collecting evaluate (from tensorrt-llm==0.19.0)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (1.3.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (8.1.8)\n",
      "Collecting click_option_group (from tensorrt-llm==0.19.0)\n",
      "  Downloading click_option_group-0.5.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aenum (from tensorrt-llm==0.19.0)\n",
      "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (26.2.0)\n",
      "Collecting fastapi==0.115.4 (from tensorrt-llm==0.19.0)\n",
      "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn (from tensorrt-llm==0.19.0)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (0.28.1)\n",
      "Requirement already satisfied: setuptools<80 in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (75.8.0)\n",
      "Collecting ordered-set (from tensorrt-llm==0.19.0)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (0.7.1)\n",
      "Collecting einops (from tensorrt-llm==0.19.0)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting flashinfer-python~=0.2.3 (from tensorrt-llm==0.19.0)\n",
      "  Downloading flashinfer_python-0.2.5.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (4.11.0.86)\n",
      "Collecting xgrammar==0.1.16 (from tensorrt-llm==0.19.0)\n",
      "  Downloading xgrammar-0.1.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting backoff (from tensorrt-llm==0.19.0)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting nvtx (from tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/nvtx/nvtx-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (562 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m562.6/562.6 kB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from tensorrt-llm==0.19.0) (3.10.0)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi==0.115.4->tensorrt-llm==0.19.0)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from fastapi==0.115.4->tensorrt-llm==0.19.0) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (0.31.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.0->tensorrt-llm==0.19.0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.51.0->tensorrt-llm==0.19.0) (4.67.1)\n",
      "Collecting tiktoken (from xgrammar==0.1.16->tensorrt-llm==0.19.0)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting ninja (from xgrammar==0.1.16->tensorrt-llm==0.19.0)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.12/site-packages (from diffusers>=0.27.0->tensorrt-llm==0.19.0) (8.6.1)\n",
      "Collecting nvidia-modelopt-core==0.27.1 (from nvidia-modelopt~=0.27.0->nvidia-modelopt[torch]~=0.27.0->tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.27.1-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m192.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting rich (from nvidia-modelopt~=0.27.0->nvidia-modelopt[torch]~=0.27.0->tensorrt-llm==0.19.0)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from nvidia-modelopt~=0.27.0->nvidia-modelopt[torch]~=0.27.0->tensorrt-llm==0.19.0) (1.15.1)\n",
      "Collecting torchprofile>=0.0.4 (from nvidia-modelopt[torch]~=0.27.0->tensorrt-llm==0.19.0)\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.12/site-packages (from onnx>=1.12.0->tensorrt-llm==0.19.0) (5.28.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9.1->tensorrt-llm==0.19.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9.1->tensorrt-llm==0.19.0) (2.27.2)\n",
      "Collecting tensorrt_cu12==10.9.0.34 (from tensorrt~=10.9.0->tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12/tensorrt_cu12-10.9.0.34.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu12_libs==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt~=10.9.0->tensorrt-llm==0.19.0)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12-libs/tensorrt_cu12_libs-10.9.0.34-py2.py3-none-manylinux_2_28_x86_64.whl (3103.3 MB)\n",
      "\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/3.1 GB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://pypi.nvidia.com \\\n",
    "            tensorrt-llm==0.19.0  --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py, tensorrt_llm as trt, torch\n",
    "print(\"mpi4py:\", mpi4py.__version__)\n",
    "print(\"TRT‚ÄëLLM:\", trt.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export trtllm \\\n",
    "  --model ../llama2-legal-merged \\\n",
    "  --task causal-lm-with-past \\\n",
    "  --dtype fp16 \\\n",
    "  --batch-size 1 \\\n",
    "  --sequence-length 4096 \\\n",
    "  ../llama2-legal-trtllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EFv44MV748Jb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.03s/it]\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:716: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/main/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\t\t-[x] values not close enough, max diff: 0.08156418800354004 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.000244140625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0009765625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.001220703125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01171875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.001953125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01171875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00244140625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00244140625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.017578125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0029296875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0234375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0048828125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0234375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01953125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00439453125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01953125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0029296875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01953125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01953125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0037841796875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00439453125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0234375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00390625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.01171875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.005859375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00537109375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0048828125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.005859375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0068359375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00732421875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00665283203125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0059814453125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0087890625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0068359375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0059814453125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.006591796875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.0068359375 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.015625 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.00732421875 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.03125 (atol: 1e-05)\n",
      "\t\t-[x] values not close enough, max diff: 0.05859375 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 0.08156418800354004\n",
      "- present.0.key: max diff = 0.00390625\n",
      "- present.0.value: max diff = 0.000244140625\n",
      "- present.1.key: max diff = 0.00390625\n",
      "- present.1.value: max diff = 0.0009765625\n",
      "- present.2.key: max diff = 0.015625\n",
      "- present.2.value: max diff = 0.001220703125\n",
      "- present.3.key: max diff = 0.01171875\n",
      "- present.3.value: max diff = 0.001953125\n",
      "- present.4.key: max diff = 0.01171875\n",
      "- present.4.value: max diff = 0.00244140625\n",
      "- present.5.key: max diff = 0.015625\n",
      "- present.5.value: max diff = 0.00244140625\n",
      "- present.6.key: max diff = 0.017578125\n",
      "- present.6.value: max diff = 0.0029296875\n",
      "- present.7.key: max diff = 0.0234375\n",
      "- present.7.value: max diff = 0.0048828125\n",
      "- present.8.key: max diff = 0.0234375\n",
      "- present.8.value: max diff = 0.00390625\n",
      "- present.9.key: max diff = 0.01953125\n",
      "- present.9.value: max diff = 0.00439453125\n",
      "- present.10.key: max diff = 0.01953125\n",
      "- present.10.value: max diff = 0.0029296875\n",
      "- present.11.key: max diff = 0.01953125\n",
      "- present.11.value: max diff = 0.00390625\n",
      "- present.12.key: max diff = 0.01953125\n",
      "- present.12.value: max diff = 0.00390625\n",
      "- present.13.key: max diff = 0.015625\n",
      "- present.13.value: max diff = 0.0037841796875\n",
      "- present.14.key: max diff = 0.015625\n",
      "- present.14.value: max diff = 0.00439453125\n",
      "- present.15.key: max diff = 0.0234375\n",
      "- present.15.value: max diff = 0.00390625\n",
      "- present.16.key: max diff = 0.015625\n",
      "- present.16.value: max diff = 0.00390625\n",
      "- present.17.key: max diff = 0.01171875\n",
      "- present.17.value: max diff = 0.005859375\n",
      "- present.18.key: max diff = 0.015625\n",
      "- present.18.value: max diff = 0.00537109375\n",
      "- present.19.key: max diff = 0.015625\n",
      "- present.19.value: max diff = 0.0048828125\n",
      "- present.20.key: max diff = 0.015625\n",
      "- present.20.value: max diff = 0.005859375\n",
      "- present.21.key: max diff = 0.015625\n",
      "- present.21.value: max diff = 0.0068359375\n",
      "- present.22.key: max diff = 0.015625\n",
      "- present.22.value: max diff = 0.00732421875\n",
      "- present.23.key: max diff = 0.015625\n",
      "- present.23.value: max diff = 0.00665283203125\n",
      "- present.24.key: max diff = 0.015625\n",
      "- present.24.value: max diff = 0.0059814453125\n",
      "- present.25.key: max diff = 0.015625\n",
      "- present.25.value: max diff = 0.0087890625\n",
      "- present.26.key: max diff = 0.015625\n",
      "- present.26.value: max diff = 0.0068359375\n",
      "- present.27.key: max diff = 0.015625\n",
      "- present.27.value: max diff = 0.0059814453125\n",
      "- present.28.key: max diff = 0.015625\n",
      "- present.28.value: max diff = 0.006591796875\n",
      "- present.29.key: max diff = 0.015625\n",
      "- present.29.value: max diff = 0.0068359375\n",
      "- present.30.key: max diff = 0.015625\n",
      "- present.30.value: max diff = 0.00732421875\n",
      "- present.31.key: max diff = 0.03125\n",
      "- present.31.value: max diff = 0.05859375.\n",
      " The exported model was saved at: ../llama2-legal-onnx\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "  --model ../llama2-legal-merged \\\n",
    "  --task causal-lm-with-past \\\n",
    "  --dtype fp16 \\\n",
    "  --device cuda \\\n",
    "  --library transformers \\\n",
    "  ../llama2-legal-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G7Rz5tQ6fCr"
   },
   "source": [
    "# Graph-optimise & kernel-fuse (Didnot Run for just checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXcrWtCj5ADx"
   },
   "outputs": [],
   "source": [
    "python -m onnxruntime_tools.optimizer_cli \\\n",
    "       --input LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder.onnx \\\n",
    "       --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
    "       --float16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjkeXn7x6lXd"
   },
   "source": [
    "# (Optional) INT-4 / INT-8 quantisation (Didnot Run for just checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0pqVRsd6Sx-"
   },
   "outputs": [],
   "source": [
    "pip install neural-compressor\n",
    "\n",
    "inc_quantizer \\\n",
    "  --model LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnx \\\n",
    "  --output LLM_LegalDocSummarization/llama2-legal-onnx/model_decoder_opt.onnxmodel_decoder_int4.onnx \\\n",
    "  --approach static  --performance-only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWvrsduA6tg7"
   },
   "source": [
    "# Quick test in ONNX Runtime (Dont Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-ypBLHI6UMJ"
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort, numpy as np, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/home/cc/models/llama2-legal-merged\")\n",
    "sess = ort.InferenceSession(\n",
    "          \"/home/cc/models/llama2-legal-onnx/model_decoder_int4.onnx\",\n",
    "          providers=[\"TensorrtExecutionProvider\",\"CUDAExecutionProvider\"])\n",
    "\n",
    "prompt = tok(\"One‚Äësentence summary of clause‚ÄØ7.2:\", return_tensors=\"np\")\n",
    "outputs = sess.run(None, {\"input_ids\":prompt[\"input_ids\"],\n",
    "                          \"attention_mask\":prompt[\"attention_mask\"]})\n",
    "print(outputs[0].shape)     # sanity: (1, seq_len, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (24.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime -y\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: "
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y nvidia-cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: CPUExecutionProvider\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     29\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (1, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     next_token_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:270\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama2-legal-merged\")\n",
    "\n",
    "# Set up ONNX Runtime session with fallback\n",
    "providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "available_providers = ort.get_available_providers()\n",
    "sess = ort.InferenceSession(\n",
    "    \"../llama2-legal-onnx/model.onnx\",\n",
    "    providers=[p for p in providers if p in available_providers]\n",
    ")\n",
    "\n",
    "print(\"Using provider:\", sess.get_providers()[0])\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"One-sentence summary of clause 7.2:\"\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Start generation loop\n",
    "max_new_tokens = 50\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    position_ids = np.arange(input_ids.shape[1], dtype=np.int64)[None, :]\n",
    "\n",
    "    outputs = sess.run(None, {\n",
    "        \"input_ids\": input_ids.astype(np.int64),\n",
    "        \"attention_mask\": attention_mask.astype(np.int64),\n",
    "        \"position_ids\": position_ids\n",
    "    })\n",
    "\n",
    "    logits = outputs[0]  # (1, seq_len, vocab_size)\n",
    "    next_token_id = np.argmax(logits[:, -1, :], axis=-1)\n",
    "\n",
    "    # Append next token\n",
    "    input_ids = np.concatenate([input_ids, next_token_id[:, None]], axis=1)\n",
    "    attention_mask = np.concatenate([attention_mask, np.ones_like(next_token_id)[:, None]], axis=1)\n",
    "\n",
    "    if next_token_id[0] == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-M1R7Su6wJo"
   },
   "source": [
    "#Wrap with FastAPI or Triton(Dont Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTEsN_6Z6WQ1"
   },
   "outputs": [],
   "source": [
    "docker run -d --gpus all -p 8000:8000 \\\n",
    "  -v /home/cc/triton_repo:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:24.05-py3 \\\n",
    "  tritonserver --model-repository=/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx5IqTOGThcA"
   },
   "source": [
    "#Build a FastAPI ONNX micro-service (pattern from the hand-out)\n",
    "````\n",
    "docker compose -f docker-compose-fastapi.yaml up -d --build\n",
    "````\n",
    "\n",
    "````\n",
    "curl -X POST http://<IP>:8000/generate \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"prompt\":\"Summarise clause¬†7.2 in two lines\"}'\n",
    "````\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
