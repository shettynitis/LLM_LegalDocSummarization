{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLZcUOG63mWO"
      },
      "source": [
        "# Merge the LoRA adapter into the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dui6RSUSLScK",
        "outputId": "988e90e9-3947-4b64-d426-2a3b702b997e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as ort\n",
        "print(ort.get_available_providers())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJZ9wEqJLScM",
        "outputId": "a0aec557-4e8f-449c-fc4e-a5b0fb71a9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.37.2\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "Collecting peft==0.7.1\n",
            "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.37.2)\n",
            "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.37.2)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
            "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.37.2)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.37.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (6.1.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.7.1) (2.5.1+cu124)\n",
            "Collecting accelerate>=0.21.0 (from peft==0.7.1)\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2)\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\n",
            "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
            "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers, accelerate, peft\n",
            "Successfully installed accelerate-1.6.0 hf-xet-1.1.0 huggingface-hub-0.31.1 peft-0.7.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.15.2 transformers-4.37.2\n"
          ]
        }
      ],
      "source": [
        "!pip install \"transformers==4.37.2\" \"peft==0.7.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h0OJ2IgLScM"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3bQo8aOLScM",
        "outputId": "ef63ab54-5f2c-4ae1-fd96-1e813d12bc36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Removing: lora_bias\n",
            "üßπ Removing: layers_to_transform\n",
            "Cleaned config saved to: ../fine_tuned_lora_model/adapter_config.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def clean_adapter_config(config_path):\n",
        "    UNNEEDED_KEYS = [\n",
        "        \"corda_config\",\n",
        "        \"eva_config\",\n",
        "        \"megatron_config\",\n",
        "        \"megatron_core\",\n",
        "        \"loftq_config\",\n",
        "        \"layers_pattern\",\n",
        "        \"layer_replication\",\n",
        "        \"auto_mapping\",\n",
        "        \"revision\",\n",
        "        \"modules_to_save\",\n",
        "        \"trainable_token_indices\",\n",
        "        \"use_dora\",\n",
        "        \"use_rslora\",\n",
        "        \"rank_pattern\",\n",
        "        \"fan_in_fan_out\",\n",
        "        \"init_lora_weights\",\n",
        "        \"exclude_modules\",\n",
        "        \"lora_bias\",\n",
        "        \"layers_to_transform\"\n",
        "    ]\n",
        "\n",
        "    path = Path(config_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Config file not found: {path}\")\n",
        "\n",
        "    with open(path, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    for key in UNNEEDED_KEYS:\n",
        "        if key in config:\n",
        "            print(f\"üßπ Removing: {key}\")\n",
        "            config.pop(key)\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"Cleaned config saved to: {path}\")\n",
        "\n",
        "# Clean this config before merging LoRA\n",
        "clean_adapter_config(\"../fine_tuned_lora_model/adapter_config.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "36e8b282e0ff46e49287c1a7b61c8a5a"
          ]
        },
        "id": "t2HI4o0CLScN",
        "outputId": "f41e0c92-8c56-4d2f-b41b-a711e997a5f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e8b282e0ff46e49287c1a7b61c8a5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('../llama2-legal-merged/tokenizer_config.json',\n",
              " '../llama2-legal-merged/special_tokens_map.json',\n",
              " '../llama2-legal-merged/tokenizer.json')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import pathlib\n",
        "\n",
        "BASE = \"meta-llama/Llama-2-7b-hf\"\n",
        "ADAPTER = \"../fine_tuned_lora_model\"\n",
        "MERGED = pathlib.Path(\"../llama2-legal-merged\")\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, ADAPTER)\n",
        "\n",
        "# ‚ö†Ô∏è MANUAL LoRA MERGE\n",
        "model.base_model.merge_and_unload()\n",
        "\n",
        "config = AutoConfig.from_pretrained(BASE)\n",
        "config.save_pretrained(MERGED)\n",
        "\n",
        "# Save the merged model\n",
        "model.save_pretrained(MERGED, safe_serialization=False)\n",
        "tokenizer.save_pretrained(MERGED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MKz8LyYLScN"
      },
      "outputs": [],
      "source": [
        "!mv ../llama2-legal-merged/adapter_model.bin ../llama2-legal-merged/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4tu5w3C6chj"
      },
      "source": [
        "# Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqrPsIm9LScN",
        "outputId": "e1da1a00-aaa2-4d68-ab06-447e07fb8298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optimum[exporters]\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (4.37.2)\n",
            "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (24.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (0.31.1)\n",
            "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (from optimum[exporters]) (1.17.0)\n",
            "Collecting onnxruntime (from optimum[exporters])\n",
            "  Downloading onnxruntime-1.21.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting timm (from optimum[exporters])\n",
            "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.2)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (1.1.0)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11->optimum[exporters]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11->optimum[exporters]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.29->optimum[exporters]) (0.5.3)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.12/site-packages (from onnx->optimum[exporters]) (5.28.3)\n",
            "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime->optimum[exporters]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime->optimum[exporters]) (25.2.10)\n",
            "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from timm->optimum[exporters]) (0.20.1+cu124)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime->optimum[exporters]) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11->optimum[exporters]) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.12.14)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision->timm->optimum[exporters]) (11.1.0)\n",
            "Downloading onnxruntime-1.21.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: onnxruntime, timm, optimum\n",
            "Successfully installed onnxruntime-1.21.1 optimum-1.24.0 timm-1.0.15\n"
          ]
        }
      ],
      "source": [
        "#tensorrt_llm==0.9.0\n",
        "pip install \"optimum-nvidia[trtllm]>=1.18.0\"              # adds `export trtllm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFv44MV748Jb",
        "outputId": "c60234c5-1fe8-4e48-a874-46b58f9e1a81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/bin/optimum-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/optimum/commands/optimum_cli.py\", line 208, in main\n",
            "    service.run()\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/optimum/commands/export/onnx.py\", line 265, in run\n",
            "    main_export(\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/optimum/exporters/onnx/__main__.py\", line 305, in main_export\n",
            "    model = TasksManager.get_model_from_task(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/optimum/exporters/tasks.py\", line 2283, in get_model_from_task\n",
            "    model = model_class.from_pretrained(model_name_or_path, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3850, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4259, in _load_pretrained_model\n",
            "    state_dict = load_state_dict(shard_file)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 513, in load_state_dict\n",
            "    return safe_load_file(checkpoint_file)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/safetensors/torch.py\", line 315, in load_file\n",
            "    result[k] = f.get_tensor(k)\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 219.81 MiB is free. Process 11179 has 26.17 GiB memory in use. Process 60944 has 13.09 GiB memory in use. Of the allocated memory 12.61 GiB is allocated by PyTorch, and 1.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "!optimum-cli export trtllm \\\n",
        "  --model ../llama2-legal-merged \\        # the LoRA‚Äëmerged FP16 HF folder\n",
        "  --task causal-lm-with-past \\            # keeps KV‚Äëcache graph\n",
        "  --dtype fp16 \\                          # FP16 weights (best perf, no extra VRAM)\n",
        "  --sequence-length 4096 \\                # max prompt length you need\n",
        "  --batch-size 1 \\                        # build vars baked into the engine\n",
        "  --device cuda \\\n",
        "  --library transformers \\\n",
        "  --output-dir ../llama2-legal-trtllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!trtllm-build \\\n",
        "  --checkpoint_dir ../llama2-legal-trtllm \\\n",
        "  --output_dir     ../llama2-legal-engine \\\n",
        "  --dtype          fp16 \\\n",
        "  --max_batch_size 1 \\\n",
        "  --max_input_len 4096 \\\n",
        "  --max_seq_len    4096 \\\n",
        "  --tp_size        1 \\\n",
        "  --enable_kv_cache"
      ],
      "metadata": {
        "id": "ZHByqhnuMXu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCrbhGQILScO",
        "outputId": "d40f6f61-d0f4-438b-920c-a78ecaa68286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{10}\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "\n",
        "model = onnx.load(\"../llama2-legal-onnx/model.onnx\")\n",
        "print({tensor.data_type for tensor in model.graph.initializer})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWvrsduA6tg7"
      },
      "source": [
        "# Quick test in ONNX Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjVlCQI8LScP",
        "outputId": "1d1ab784-8a5e-443e-ac72-210c0bfe5ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "['CPUExecutionProvider']\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as ort\n",
        "print(ort.get_available_providers())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm.runtime import GenerationSession        # :contentReference[oaicite:2]{index=2}\n",
        "from transformers import AutoTokenizer\n",
        "import time\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"../llama2-legal-merged\")\n",
        "\n",
        "session = GenerationSession(\n",
        "    engine_dir     =\"../llama2-legal-engine\",\n",
        "    max_new_tokens =64,                   # runtime override\n",
        "    dtype          =\"float16\",\n",
        ")\n",
        "\n",
        "prompt = \"One‚Äësentence summary of clause 7.2:\"\n",
        "start = time.time()\n",
        "outputs = session.generate([prompt])      # list‚Äëin, list‚Äëout\n",
        "print(outputs[0])\n",
        "print(\"Latency:\", time.time()-start, \"s\")"
      ],
      "metadata": {
        "id": "HvSixLy7Mem8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-M1R7Su6wJo"
      },
      "source": [
        "#Wrap with FastAPI or Triton(Dont Run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTEsN_6Z6WQ1"
      },
      "outputs": [],
      "source": [
        "docker run -d --gpus all -p 8000:8000 \\\n",
        "  -v /home/cc/triton_repo:/models \\\n",
        "  nvcr.io/nvidia/tritonserver:24.05-py3 \\\n",
        "  tritonserver --model-repository=/models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx5IqTOGThcA"
      },
      "source": [
        "#Build a FastAPI ONNX micro-service (pattern from the hand-out)\n",
        "````\n",
        "docker compose -f docker-compose-fastapi.yaml up -d --build\n",
        "````\n",
        "\n",
        "````\n",
        "curl -X POST http://<IP>:8000/generate \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -d '{\"prompt\":\"Summarise clause¬†7.2 in two lines\"}'\n",
        "````\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}