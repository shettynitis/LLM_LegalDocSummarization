{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Getting data from source"
      ],
      "metadata": {
        "id": "Nq8pXLQ5mMF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mQK-akRlQhtH"
      },
      "outputs": [],
      "source": [
        "# Download & unzip the Zenodo archive\n",
        "!wget -q \"https://zenodo.org/record/7152317/files/dataset.zip?download=1\" -O dataset.zip\n",
        "!unzip -q dataset.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "bhitNU8kQ0lJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the data to Normalize Unicode, collapse whitespace, lowercase, strip boilerplate"
      ],
      "metadata": {
        "id": "gQnRwRBimQ_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    # Normalize Unicode, collapse whitespace, lowercase, strip boilerplate\n",
        "    text = unicodedata.normalize(\"NFKC\", text).strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"page \\d+ of \\d+\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "qi4YJD-oihtD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the three datasources"
      ],
      "metadata": {
        "id": "pSSL8TZ7mVBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_ROOT = \"data/dataset\"       # location where we unzipped Zenodo\n",
        "OUT       = \"merged_dataset\"     # final output folder\n",
        "\n",
        "# â”€â”€ Ensure the merged_dataset folder exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# â”€â”€ Prepare for gathering cases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cases   = []\n",
        "TMP_DIR = \"/tmp\"  # where weâ€™ll write merged segment summaries\n",
        "\n",
        "def gather(folder, doc_sub, sum_sub, full_sub=None):\n",
        "    \"\"\"\n",
        "    Populates `cases` with (doc_path, summary_path, filename).\n",
        "    For IN-Ext, tries full_sub first, else concatenates segment-wise.\n",
        "    \"\"\"\n",
        "    base   = os.path.join(DATA_ROOT, folder)\n",
        "    splits = (\"train-data\",\"test-data\") if folder != \"IN-Ext\" else (\"\",)\n",
        "\n",
        "    for sd in splits:\n",
        "        src      = os.path.join(base, sd) if sd else base\n",
        "        docs_dir = os.path.join(src, doc_sub)\n",
        "        if not os.path.isdir(docs_dir):\n",
        "            continue\n",
        "\n",
        "        for fn in os.listdir(docs_dir):\n",
        "            if not fn.endswith(\".txt\"):\n",
        "                continue\n",
        "\n",
        "            doc_path     = os.path.join(docs_dir, fn)\n",
        "            summary_path = None\n",
        "\n",
        "            if folder == \"IN-Ext\":\n",
        "                # 1) Try full/A1\n",
        "                candidate = os.path.join(src, sum_sub, full_sub, fn)\n",
        "                if os.path.isfile(candidate):\n",
        "                    summary_path = candidate\n",
        "                else:\n",
        "                    # 2) Fallback: merge segment-wise A1\n",
        "                    segments = [\"analysis\",\"argument\",\"facts\",\"judgement\",\"statute\"]\n",
        "                    pieces = []\n",
        "                    for seg in segments:\n",
        "                        seg_file = os.path.join(src, \"summary\", \"segment-wise\", full_sub, seg, fn)\n",
        "                        if os.path.isfile(seg_file):\n",
        "                            with open(seg_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                                pieces.append(f.read())\n",
        "                    if pieces:\n",
        "                        tmp_file = os.path.join(TMP_DIR, fn)\n",
        "                        with open(tmp_file, \"w\", encoding=\"utf-8\") as out:\n",
        "                            out.write(\"\\n\\n\".join(pieces))\n",
        "                        summary_path = tmp_file\n",
        "\n",
        "            else:\n",
        "                # IN-Abs / UK-Abs: either full_sub or direct summary\n",
        "                if full_sub:\n",
        "                    fullp   = os.path.join(src, sum_sub, full_sub, fn)\n",
        "                    direct  = os.path.join(src, sum_sub, fn)\n",
        "                    summary_path = fullp if os.path.isfile(fullp) else (direct if os.path.isfile(direct) else None)\n",
        "                else:\n",
        "                    direct  = os.path.join(src, sum_sub, fn)\n",
        "                    summary_path = direct if os.path.isfile(direct) else None\n",
        "\n",
        "            if summary_path:\n",
        "                cases.append((doc_path, summary_path, fn))\n",
        "\n",
        "# â”€â”€ Gather from each source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "gather(\"IN-Abs\", \"judgement\", \"summary\")                  # IN-Abs\n",
        "gather(\"UK-Abs\", \"judgement\", \"summary\", full_sub=\"full\") # UK-Abs\n",
        "gather(\"IN-Ext\", \"judgement\", \"summary\", full_sub=\"A1\")   # IN-Ext"
      ],
      "metadata": {
        "id": "RnYXmUbQQ7GU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check for duplicated, empty texts"
      ],
      "metadata": {
        "id": "QtYJ5vY0mgxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity-check: every pair exists and is non-empty\n",
        "\n",
        "clean_cases = []\n",
        "for doc, summ, fn in cases:\n",
        "    if os.path.getsize(doc) == 0 or os.path.getsize(summ) == 0:\n",
        "        print(f\"Skipping empty file: {fn}\")\n",
        "    else:\n",
        "        clean_cases.append((doc, summ, fn))\n",
        "cases = clean_cases\n",
        "print(f\"{len(cases)} cases retained after dropping empty files.\")\n",
        "\n",
        "# 2) Assert all remaining files exist\n",
        "missing = [fn for doc, summ, fn in cases\n",
        "           if not (os.path.isfile(doc) and os.path.isfile(summ))]\n",
        "assert not missing, f\"âŒ Missing files: {missing}\"\n",
        "\n",
        "# 3) Assert no duplicate filenames\n",
        "dupes = [fn for fn, cnt in Counter(fn for _, _, fn in cases).items() if cnt > 1]\n",
        "assert not dupes, f\"âŒ Duplicate filenames: {dupes}\"\n",
        "\n",
        "print(\"âœ… Sanity check passed: all files present, non-empty, and unique.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4thEkfsj6U1",
        "outputId": "3b675804-3634-4b51-dd50-ac2db005a680"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping empty file: 4799.txt\n",
            "Skipping empty file: 299.txt\n",
            "7971 cases retained after dropping empty files.\n",
            "âœ… Sanity check passed: all files present, non-empty, and unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop outliers - Drop any case whose summary is under 50 words, over 1500 words, or whose summary / document word count ratio is outside 0.01, 0.5"
      ],
      "metadata": {
        "id": "k-K_EfXtm7fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the helper\n",
        "def get_stats(txt: str):\n",
        "    \"\"\"Return (word_count, sentence_count).\"\"\"\n",
        "    words = len(txt.split())\n",
        "    # naive sentence split on [.?!] + whitespace\n",
        "    sents = len([s for s in re.split(r'[\\.!?]\\s+', txt) if s.strip()])\n",
        "    return words, sents\n",
        "\n",
        "# Apply statsâ€based filtering independently\n",
        "stats_filtered = []\n",
        "for doc_path, sum_path, fn in cases:\n",
        "    # read & clean (reuse your clean_text)\n",
        "    raw_doc = open(doc_path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    raw_sum = open(sum_path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    doc_clean = clean_text(raw_doc)\n",
        "    sum_clean = clean_text(raw_sum)\n",
        "\n",
        "    # compute stats\n",
        "    dw, ds = get_stats(doc_clean)\n",
        "    sw, ss = get_stats(sum_clean)\n",
        "    ratio = sw / dw if dw > 0 else 0\n",
        "\n",
        "    # outlier check\n",
        "    if sw < 50 or sw > 1500 or not (0.01 <= ratio <= 0.5):\n",
        "        # drop this case\n",
        "        continue\n",
        "\n",
        "    # keep if it passes\n",
        "    stats_filtered.append((doc_path, sum_path, fn))\n",
        "\n",
        "# Replace cases list\n",
        "cases = stats_filtered\n",
        "print(f\"{len(cases)} cases remain after word/sentenceâ€count filtering.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTsMMZkkXqv",
        "outputId": "41efa10a-5f23-43b8-ab3a-0c723816ef86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7035 cases remain after word/sentenceâ€count filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize and split into train, test, production (70-20-10)"
      ],
      "metadata": {
        "id": "2Ib4XWLsnF32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle & split 70/20/10\n",
        "random.shuffle(cases)\n",
        "N       = len(cases)\n",
        "n_train = int(0.7 * N)\n",
        "n_test  = int(0.2 * N)\n",
        "\n",
        "splits = [\n",
        "    (\"train\",      cases[:n_train]),\n",
        "    (\"test\",       cases[n_train:n_train+n_test]),\n",
        "    (\"production\", cases[n_train+n_test:])\n",
        "]"
      ],
      "metadata": {
        "id": "3_BlwYPLRA_G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to JSONL"
      ],
      "metadata": {
        "id": "mV_d4AjenNFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Dump one JSONL per split with metadata baked in\n",
        "for split, subset in splits:\n",
        "    out_path = os.path.join(OUT, f\"{split}.jsonl\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for doc_path, summ_path, fn in subset:\n",
        "            # Read raw files\n",
        "            raw_doc = open(doc_path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "            raw_sum = open(summ_path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "\n",
        "            # Clean\n",
        "            doc_clean = clean_text(raw_doc)\n",
        "            sum_clean = clean_text(raw_sum)\n",
        "\n",
        "            # Stats + outlier filter\n",
        "            dw, ds = get_stats(doc_clean)\n",
        "            sw, ss = get_stats(sum_clean)\n",
        "            ratio = sw / dw if dw else 0\n",
        "            if sw < 50 or sw > 1500 or not (0.01 <= ratio <= 0.5):\n",
        "                continue\n",
        "\n",
        "            # Build & write record\n",
        "            record = {\n",
        "                \"filename\":  fn,\n",
        "                \"judgement\": doc_clean,\n",
        "                \"summary\":   sum_clean,\n",
        "                \"meta\": {\n",
        "                    \"doc_words\": dw, \"doc_sents\": ds,\n",
        "                    \"sum_words\": sw, \"sum_sents\": ss,\n",
        "                    \"ratio\":     ratio\n",
        "                }\n",
        "            }\n",
        "            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # Confirm how many you wrote\n",
        "    count = sum(1 for _ in open(out_path, \"r\", encoding=\"utf-8\"))\n",
        "    print(f\"ðŸ“¦ {split:10s} â†’ {count} records to {out_path}\")\n"
      ],
      "metadata": {
        "id": "JcMcDfqXRC_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303f3a1e-3a8f-4a02-b2f6-05f883de0fa5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ train      â†’ 4924 records to merged_dataset/train.jsonl\n",
            "ðŸ“¦ test       â†’ 1407 records to merged_dataset/test.jsonl\n",
            "ðŸ“¦ production â†’ 704 records to merged_dataset/production.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing to Git repository"
      ],
      "metadata": {
        "id": "UaRksfHepNse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your PAT so it isnâ€™t visible in the notebook\n",
        "os.environ[\"GITHUB_PAT\"] = \"\"\n",
        "\n",
        "# Clone your fork, not the upstream\n",
        "!git clone https://$GITHUB_PAT@github.com/robo-ro/LLM_LegalDocSummarization.git\n",
        "%cd LLM_LegalDocSummarization\n",
        "\n",
        "# Configure Git identity\n",
        "!git config user.email \"you@example.com\"\n",
        "!git config user.name  \"Your Name\"\n",
        "\n",
        "# Create & switch to your branch\n",
        "!git checkout -b add-merged-dataset\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r merged_dataset.zip merged_dataset\n",
        "\n",
        "# Stage & commit the zip (instead of the raw folder)\n",
        "!git add merged_dataset.zip\n",
        "!git commit -m \"Add merged_dataset.zip containing processed JSONL data\"\n",
        "\n",
        "# Push as before\n",
        "!git push origin add-merged-dataset\n"
      ],
      "metadata": {
        "id": "oZhLgtUynmzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VH4PxvKRqK_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}