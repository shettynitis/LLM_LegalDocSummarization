{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit jobs to the Ray cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray Train\n",
    "\n",
    "\n",
    "\n",
    "``` bash\n",
    "# run in a terminal inside jupyter container\n",
    "cd ~/work\n",
    "git stash # stash any changes you made to the current branch\n",
    "git fetch -a\n",
    "git switch ray\n",
    "cd ~/work\n",
    "```\n",
    "\n",
    "\n",
    "``` bash\n",
    "# runs on jupyter container inside node-mltrain, from inside the \"work\" directory\n",
    "ray job submit --runtime-env runtime.json  --working-dir .  -- python gourmetgram-train/train.py \n",
    "```\n",
    "\n",
    "Submit the job, and note that it runs mostly as before. Let it run until it is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use Ray Train with multiple workers\n",
    "\n",
    "\n",
    "``` python\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True, resources_per_worker={\"GPU\": 1, \"CPU\": 8})\n",
    "```\n",
    "\n",
    "to scale to two worker nodes, each with 1 GPU and 8 GPUs assigned to the job. Save it, and run with\n",
    "\n",
    "``` bash\n",
    "# runs on jupyter container inside node-mltrain, from inside the \"work\" directory\n",
    "ray job submit --runtime-env runtime.json  --working-dir .  -- python gourmetgram-train/train.py \n",
    "```\n",
    "\n",
    "On the Ray dashboard, in the “Resource Status” section of the “Overview” tab, you should see the increased resource requirements reflected in the “Usage” section.\n",
    "\n",
    "In a terminal on the “node-mltrain” host (*not* inside the Jupyter container), run\n",
    "\n",
    "``` bash\n",
    "# runs on node-mltrain\n",
    "nvtop\n",
    "```\n",
    "\n",
    "and confirm that both GPUs are busy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray Train for fault tolerance\n",
    "\n",
    "Next, let’s try out fault tolerance! If the worker node that runs our Ray Train job dies, Ray can resume from the most recent checkpoint on another worker node.\n",
    "\n",
    "Fault tolerance is configured in another branch\n",
    "\n",
    "``` bash\n",
    "# run in a terminal inside jupyter container\n",
    "cd ~/work/gourmetgram-train\n",
    "git stash # stash any changes you made to the current branch\n",
    "git fetch -a\n",
    "git switch fault_tolerance\n",
    "cd ~/work\n",
    "```\n",
    "\n",
    "To add fault tolerance, we\n",
    "\n",
    "-   have an additional import\n",
    "-   add it to our `RunConfig`:\n",
    "\n",
    "``` python\n",
    "run_config = RunConfig( ... failure_config=FailureConfig(max_failures=2))\n",
    "```\n",
    "\n",
    "-   and inside `train_fun`, we replace the old\n",
    "\n",
    "``` python\n",
    "trainer.fit(lightning_food11_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "```\n",
    "\n",
    "with\n",
    "\n",
    "``` python\n",
    "## For Ray Train fault tolerance with FailureConfig\n",
    "# Recover from checkpoint, if we are restoring after failure\n",
    "checkpoint = train.get_checkpoint()\n",
    "if checkpoint:\n",
    "    with checkpoint.as_directory() as ckpt_dir:\n",
    "        ckpt_path = os.path.join(ckpt_dir, \"checkpoint.ckpt\")\n",
    "        trainer.fit(lightning_food11_model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=ckpt_path)\n",
    "else:\n",
    "        trainer.fit(lightning_food11_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Ray system\n",
    "\n",
    "\n",
    "\n",
    "For NVIDIA GPUs:\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker compose -f LLM_LegalDocSummarization/docker/docker-compose-ray-cuda.yaml down\n",
    "```\n",
    "\n",
    "and then stop the Jupyter server with\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker stop jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
